{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1s5FLjgkCmVCgBRugvJp-9kyvUv_f3aEl",
      "authorship_tag": "ABX9TyPtxJySxWMawDQ+/Y9YmaXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Silmar123/News_project/blob/main/Reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nc7WlyoulZg",
        "outputId": "959e3b17-acd1-42f8-93f0-16a6743d35c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install -q praw\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 153kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 9.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsyONxk0iQJc"
      },
      "source": [
        "# Load libraries and login profile "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tktCzv4s05g1"
      },
      "source": [
        "import praw\n",
        "import json\n",
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"7f_KCgYipcdlRQ\",\n",
        "    client_secret=\"pQ3pJbSMA-j8iGEhTKRHbTWC-UE\",\n",
        "    user_agent=\"Project_news_follow\",\n",
        "    username=\"Silmar_123\",\n",
        "    password=\"Fuego123\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_RTL3W5imDT"
      },
      "source": [
        "# List of subreddits to explore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTV0cBWL_Ff9"
      },
      "source": [
        "#List of subreddits topics General news / politics\n",
        "\n",
        "subreddit_news_politic_1 = \"es\"\n",
        "subreddit_news_politic_2 = \"spain\"\n",
        "subreddit_news_politic_3 = \"ActualidadMundial\"\n",
        "subreddit_news_politic_4 = \"Asi_va_Espana\"\n",
        "subreddit_news_politic_5 = \"MeneamePortada\"\n",
        "subreddit_news_politic_6 = \"podemos\"\n",
        "subreddit_news_politic_7 = \"vox_spain\"\n",
        "subreddit_news_politic_8 = \"SpainPolitics\"\n",
        "subreddit_news_politic_9 = \"VivaEspana\"\n",
        "subreddit_news_politic_10 = \"PSOE\"\n",
        "\n",
        "\n",
        "subreddit_news_politc_red = [subreddit_news_politic_1,subreddit_news_politic_2,subreddit_news_politic_3,subreddit_news_politic_4,subreddit_news_politic_5,\n",
        "                             subreddit_news_politic_6,subreddit_news_politic_7,subreddit_news_politic_8,subreddit_news_politic_9,subreddit_news_politic_10]\n",
        "\n",
        "#List of subreddits topics  meme\n",
        "\n",
        "subreddit_meme_1 = \"EPANA\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMCe7ikKiuKb"
      },
      "source": [
        "# Dictionary to store data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy_IlFKhtRRm"
      },
      "source": [
        "#Create a dictionry to introduce de news:\n",
        "\n",
        "list_of_keys = [\"Id del post\",\"Título\",\"ups\",\"downs\",\"Número de comentarios\",\"Autor\",\"Fecha\",\"link\",\"subreddit\"]\n",
        "\n",
        "news_reddit = dict.fromkeys(list_of_keys)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLXE0Fv6i04y"
      },
      "source": [
        "# Scrapping articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWqPkoRp3hT6"
      },
      "source": [
        "#Number of submissions to scrap\n",
        "\n",
        "limit = 100\n",
        "\n",
        "#Define an empty list to every key\n",
        "for keys in list_of_keys:\n",
        "  news_reddit[keys] = []\n",
        "\n",
        "#For loop that goes over the subredits\n",
        "\n",
        "for subreddit_news_politc in subreddit_news_politc_red:\n",
        "\n",
        "  subred = reddit.subreddit(subreddit_news_politc)\n",
        "  new_subreddit = subred.new(limit = limit)\n",
        "\n",
        "\n",
        "#Informatin \n",
        "  for submission in new_subreddit:\n",
        "\n",
        "    if not submission.stickied:\n",
        "\n",
        "     news_reddit[\"Id del post\"].append(submission.id)\n",
        "\n",
        "     news_reddit[\"Título\"].append(submission.title)\n",
        "\n",
        "     news_reddit[\"ups\"].append(submission.ups)\n",
        "\n",
        "     news_reddit[\"downs\"].append(submission.downs)\n",
        "\n",
        "     news_reddit[\"Número de comentarios\"].append(submission.num_comments)\n",
        "\n",
        "     news_reddit[\"Autor\"].append(submission.author)\n",
        "\n",
        "     news_reddit[\"Fecha\"].append(submission.created_utc)\n",
        "\n",
        "     news_reddit[\"link\"].append(submission.url)\n",
        "\n",
        "     news_reddit[\"subreddit\"].append(subreddit_news_politc)\n",
        "\n",
        "     \n",
        "\n",
        "     \n",
        "\n",
        "     \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eABOKQJeKKF-"
      },
      "source": [
        "## Function to Scrap Submissions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apVwYd14M_6H"
      },
      "source": [
        "Returns a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDyr0Wp4KOw6"
      },
      "source": [
        "def Scrap_submissions(subreddit_list,number_of_submissions):\n",
        "  \n",
        "  list_of_keys = [\"Id del post\",\"Título\",\"ups\",\"downs\",\"Número de comentarios\",\"Autor\",\"Fecha\",\"link\",\"subreddit\"]\n",
        "\n",
        "  news_reddit = dict.fromkeys(list_of_keys)\n",
        "\n",
        "  #Number of submissions to scrap\n",
        "\n",
        "  limit = number_of_submissions\n",
        "\n",
        "  #Define an empty list to every key\n",
        "  for keys in list_of_keys:\n",
        "    news_reddit[keys] = []\n",
        "\n",
        "#For loop that goes over the subredits\n",
        "\n",
        "  for subreddit_submission in subreddit_list:\n",
        "\n",
        "    subred = reddit.subreddit(subreddit_news_politc)\n",
        "    new_subreddit = subred.new(limit = limit)\n",
        "\n",
        "\n",
        "#Informatin \n",
        "    for submission in new_subreddit:\n",
        "\n",
        "      if not submission.stickied:\n",
        "\n",
        "        news_reddit[\"Id del post\"].append(submission.id)\n",
        "\n",
        "        news_reddit[\"Título\"].append(submission.title)\n",
        "\n",
        "        news_reddit[\"ups\"].append(submission.ups)\n",
        "\n",
        "        news_reddit[\"downs\"].append(submission.downs)\n",
        "\n",
        "        news_reddit[\"Número de comentarios\"].append(submission.num_comments)\n",
        "\n",
        "        news_reddit[\"Autor\"].append(submission.author)\n",
        "\n",
        "        news_reddit[\"Fecha\"].append(submission.created_utc)\n",
        "\n",
        "        news_reddit[\"link\"].append(submission.url)\n",
        "\n",
        "        news_reddit[\"subreddit\"].append(subreddit_news_politc)\n",
        "\n",
        "  return(news_reddit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKoq5CJl6H1H"
      },
      "source": [
        "# From unix to day time\n",
        "\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "from datetime import date\n",
        "\n",
        "for date_post in news_reddit[\"Fecha\"]:\n",
        "\n",
        "  hora_unix = int(date_post)\n",
        "\n",
        "  hora_madrid_delta = 2\n",
        "\n",
        "  anadir_hora = timedelta(hours = hora_madrid_delta)\n",
        "\n",
        "  fecha_publicado = datetime.utcfromtimestamp(hora_unix) + anadir_hora\n",
        "\n",
        "  fecha_publicado = fecha_publicado.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "  index_date = news_reddit[\"Fecha\"].index(date_post)\n",
        "\n",
        "  news_reddit[\"Fecha\"][index_date] = fecha_publicado \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV8HC_-Itu0k"
      },
      "source": [
        "def unix_time_madrid(time_unix):\n",
        "\n",
        "  hora_unix = int(time_unix)\n",
        "\n",
        "  hora_madrid_delta = 2\n",
        "\n",
        "  anadir_hora = timedelta(hours = hora_madrid_delta)\n",
        "\n",
        "  date_time = datetime.utcfromtimestamp(hora_unix) + anadir_hora\n",
        "\n",
        "  date_time = date_time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "  return(date_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xipBbs0ujKiy"
      },
      "source": [
        "## Giving data frame structure and saving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA_kl74Rwqx3"
      },
      "source": [
        "#Transform the dictionary to dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df_reddit_news = pd.DataFrame(news_reddit)  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF6nrL_gMmL4",
        "outputId": "71d2ca5b-e155-4942-faa9-86c1cd0c3f9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "#Visualizing data frame\n",
        "df_reddit_news.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id del post</th>\n",
              "      <th>Título</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>Número de comentarios</th>\n",
              "      <th>Autor</th>\n",
              "      <th>Fecha</th>\n",
              "      <th>link</th>\n",
              "      <th>subreddit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jfm533</td>\n",
              "      <td>Echenique, condenado a pagar 11.040 euros por ...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Moztruitu</td>\n",
              "      <td>2020-10-21 23:29:49</td>\n",
              "      <td>https://www.republica.com/2020/10/21/echenique...</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>jfjju4</td>\n",
              "      <td>El arte de la tapa murciana</td>\n",
              "      <td>156</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>tuidelescribano</td>\n",
              "      <td>2020-10-21 21:16:16</td>\n",
              "      <td>https://i.redd.it/hmde0fhnzhu51.jpg</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jfj7q0</td>\n",
              "      <td>Maribel Guardia en defensa de la India María s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>edwinn_herrera</td>\n",
              "      <td>2020-10-21 20:59:35</td>\n",
              "      <td>https://noticiasnrt.com/2020/10/21/maribel-gua...</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jfcyhb</td>\n",
              "      <td>Goldman quiere devolver viviendas sociales per...</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>JoseCarlos1976</td>\n",
              "      <td>2020-10-21 15:38:26</td>\n",
              "      <td>https://www.elboletin.com/noticia/199733/merca...</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jfc4q7</td>\n",
              "      <td>Abascal ofrece convocar elecciones antes de qu...</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>kaelne</td>\n",
              "      <td>2020-10-21 14:47:52</td>\n",
              "      <td>https://www.elmundo.es/espana/2020/10/21/5f8fd...</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Id del post  ... subreddit\n",
              "0      jfm533  ...        es\n",
              "1      jfjju4  ...        es\n",
              "2      jfj7q0  ...        es\n",
              "3      jfcyhb  ...        es\n",
              "4      jfc4q7  ...        es\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnMnUq63fPG4"
      },
      "source": [
        "#Getting the date of today\n",
        "\n",
        "now = datetime.now()\n",
        "now = now+anadir_hora#local time\n",
        "now= now.strftime(\"%d_%m_%Y__%Hh_%Mm_%Ss\")\n",
        "now_hour=now[:15]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvjaqUzUE0ly"
      },
      "source": [
        "## Save data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-qzxemZRhSN"
      },
      "source": [
        "#Write dataframe to csv\n",
        "\n",
        "df_reddit_news.to_csv(\"/content/drive/My Drive/Colab Notebooks/UOC_B/Reddit_news_\"+str(now)+\".csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiG1AWc7l710"
      },
      "source": [
        "# Getting comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC_rk0PLnQ5b"
      },
      "source": [
        "## Working with the comments for one submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQIUI9vAmHGF"
      },
      "source": [
        "\n",
        "id = \"jfc4q7\"\n",
        "\n",
        "submission = reddit.submission(id = id)\n",
        "\n",
        "submission.comments.replace_more(limit=None)\n",
        "for comment in submission.comments.list():\n",
        "    print(comment.body)\n",
        "    print(comment.id)\n",
        "    print(comment.parent_id)   \n",
        "    print(comment.score)\n",
        "    print(comment.author)\n",
        "    print(unix_time_madrid(comment.created))\n",
        "    print(20*\"-\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB-Bt-cku8L3"
      },
      "source": [
        "## To save resources, I look for the submissions with at least 10 ups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Tj0YGlvOIE"
      },
      "source": [
        "df_reddit_news[df_reddit_news[\"ups\"]>=10].sort_values(by=\"ups\",  ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgVKg3ohyY0b"
      },
      "source": [
        "#list of relevant submissions id sorted by ups\n",
        "\n",
        "relevent_ups = []\n",
        "\n",
        "for relevant_up in df_reddit_news[df_reddit_news[\"ups\"]>=10].sort_values(by=\"ups\",  ascending=False)[\"Id del post\"]:\n",
        "  relevent_ups.append(relevant_up)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5wNQIRA54HQ"
      },
      "source": [
        "#Create a dictionry to introduce de comments:\n",
        "\n",
        "list_of_keys = [\"Id del Post\",\"Comment Id\",\"Parent Id\",\"Puntuación\",\"Autor\",\"Fecha\",\"Texto\"]\n",
        "\n",
        "comment_reddit = dict.fromkeys(list_of_keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCBSUHVGEeJf"
      },
      "source": [
        "## Scrapping  comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP0Ucd0D6hAb"
      },
      "source": [
        "#Define an empty list to every key\n",
        "for keys in list_of_keys:\n",
        "  comment_reddit[keys] = []\n",
        "\n",
        "#For loop that goes over the comments\n",
        "\n",
        "for post_id in relevent_ups:\n",
        "\n",
        "  id = post_id\n",
        "\n",
        "  submission = reddit.submission(id = id)\n",
        "\n",
        "  submission.comments.replace_more(limit=None)\n",
        "\n",
        "  for comment in submission.comments.list():\n",
        "\n",
        "    comment_reddit[\"Id del Post\"].append(post_id)\n",
        "\n",
        "    comment_reddit[\"Comment Id\"].append(comment.id)\n",
        "\n",
        "    comment_reddit[\"Parent Id\"].append(comment.parent_id)\n",
        "\n",
        "    comment_reddit[\"Puntuación\"].append(comment.score)\n",
        "\n",
        "    comment_reddit[\"Autor\"].append(comment.author)\n",
        "\n",
        "    comment_reddit[\"Fecha\"].append(unix_time_madrid(comment.created))\n",
        "\n",
        "    comment_reddit[\"Texto\"].append(comment.body)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F5PvgW9L2sL"
      },
      "source": [
        "## Function to Scrap Comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCaibCIrM0DK"
      },
      "source": [
        "Returns a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDjqCZy-L8OH"
      },
      "source": [
        "def Scrap_reddit_comments(list_of_submission_id):\n",
        "\n",
        "#Create a dictionry to introduce de comments:\n",
        "\n",
        "  list_of_keys = [\"Id del Post\",\"Comment Id\",\"Parent Id\",\"Puntuación\",\"Autor\",\"Fecha\",\"Texto\"]\n",
        "\n",
        "  comment_reddit = dict.fromkeys(list_of_keys)\n",
        "\n",
        "  #Define an empty list to every key\n",
        "  for keys in list_of_keys:\n",
        "    comment_reddit[keys] = []\n",
        "\n",
        "#For loop that goes over the comments\n",
        "\n",
        "  for post_id in list_of_submission_id:\n",
        "\n",
        "    id = post_id\n",
        "\n",
        "    submission = reddit.submission(id = id)\n",
        "\n",
        "    submission.comments.replace_more(limit=None)\n",
        "\n",
        "    for comment in submission.comments.list():\n",
        "\n",
        "      comment_reddit[\"Id del Post\"].append(post_id)\n",
        "\n",
        "      comment_reddit[\"Comment Id\"].append(comment.id)\n",
        "\n",
        "      comment_reddit[\"Parent Id\"].append(comment.parent_id)\n",
        "\n",
        "      comment_reddit[\"Puntuación\"].append(comment.score)\n",
        "\n",
        "      comment_reddit[\"Autor\"].append(comment.author)\n",
        "\n",
        "      comment_reddit[\"Fecha\"].append(unix_time_madrid(comment.created))\n",
        "\n",
        "      comment_reddit[\"Texto\"].append(comment.body)\n",
        "\n",
        "  return(comment_reddit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY7DzYZVFGNB"
      },
      "source": [
        "## Giving data frame structure and saving data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCePGHE6_J6U"
      },
      "source": [
        "df_reddit_comments = pd.DataFrame(comment_reddit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjl5rRX__e4s",
        "outputId": "4765484b-39f9-4346-a30f-7ceb5a10c692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df_reddit_comments.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id del Post</th>\n",
              "      <th>Comment Id</th>\n",
              "      <th>Parent Id</th>\n",
              "      <th>Puntuación</th>\n",
              "      <th>Autor</th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Texto</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>j9lhkz</td>\n",
              "      <td>g8kiu6p</td>\n",
              "      <td>t3_j9lhkz</td>\n",
              "      <td>48</td>\n",
              "      <td>OccidentalGuy</td>\n",
              "      <td>2020-10-12 16:32:25</td>\n",
              "      <td>Thanks!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>j9lhkz</td>\n",
              "      <td>g8kj62n</td>\n",
              "      <td>t3_j9lhkz</td>\n",
              "      <td>36</td>\n",
              "      <td>drawingmentally</td>\n",
              "      <td>2020-10-12 16:37:51</td>\n",
              "      <td>Aw, thanks :)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>j9lhkz</td>\n",
              "      <td>g8kosd8</td>\n",
              "      <td>t3_j9lhkz</td>\n",
              "      <td>24</td>\n",
              "      <td>ComandanteJ50</td>\n",
              "      <td>2020-10-12 18:18:41</td>\n",
              "      <td>Ty so much, cheers to Netherlands🇳🇱🇳🇱</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>j9lhkz</td>\n",
              "      <td>g8kpr50</td>\n",
              "      <td>t3_j9lhkz</td>\n",
              "      <td>15</td>\n",
              "      <td>TheRealJuampa</td>\n",
              "      <td>2020-10-12 18:37:31</td>\n",
              "      <td>Dankje wel vriend!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>j9lhkz</td>\n",
              "      <td>g8kpsuu</td>\n",
              "      <td>t3_j9lhkz</td>\n",
              "      <td>15</td>\n",
              "      <td>Snoo10880</td>\n",
              "      <td>2020-10-12 18:38:26</td>\n",
              "      <td>Thanks friends</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Id del Post  ...                                  Texto\n",
              "0      j9lhkz  ...                                Thanks!\n",
              "1      j9lhkz  ...                          Aw, thanks :)\n",
              "2      j9lhkz  ...  Ty so much, cheers to Netherlands🇳🇱🇳🇱\n",
              "3      j9lhkz  ...                     Dankje wel vriend!\n",
              "4      j9lhkz  ...                         Thanks friends\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0aHvOXXE9jy"
      },
      "source": [
        "## Save data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhbDJL6ay7I4"
      },
      "source": [
        "#Create an appropiate folder\n",
        "\n",
        "now = datetime.now()\n",
        "now = now+anadir_hora#local time\n",
        "now= now.strftime(\"%d_%m_%Y__%Hh_%Mm_%Ss\")\n",
        "now_hour=now[:15]\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/UOC_B/reddit_comments_\"+str(now_hour)\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO0t9FdrBUtR"
      },
      "source": [
        "#Write dataframe to csv\n",
        "\n",
        "df_reddit_comments.to_csv(path+\"/Reddit_news_comments\"+str(now)+\".csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}